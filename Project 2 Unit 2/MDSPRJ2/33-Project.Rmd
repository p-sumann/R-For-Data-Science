---
title: "**Project 2**"
author: "**Suman Paudel**"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    
    highlight: pygments
    latex_engine: xelatex
    keep_tex: true
    extra_dependencies:
      caption: ["labelfont={bf}"]
      hyperref: ["unicode=true", "breaklinks=true"]
      lmodern: null
knit: (function(inputFile, encoding) {
      out_dir <- "reports";
      rmarkdown::render(inputFile,
                        encoding=encoding,
                        output_dir=file.path(dirname(inputFile), out_dir))})
---



# **\textcolor{magenta}{Task 1}**


#### **\textcolor{magenta}{Part 1}**
##### *Task 1*

#### **Load all of the necessary packages need for task 1.**
```{r warning=FALSE, cache=TRUE, message=FALSE}
# Load the packages
library(foreign) 
library(gt)
library(tidyverse)
library(magrittr)
```

###### Load the Data using CSV module from base R
``` {r cache=TRUE}
# load the data using Base R read.csv 
data <- read.csv("covnep_252days.csv")

summary(data$totalCases)
```

###### Since we need value as 1 instead of zero We can achieve this using \ multiple ways like ifelse or pmax or subsetting

###### **Using `ifelse`**
```{r cache=TRUE}
# using ifelse
totalCases_ifelse <- ifelse(data$totalCases < 1, 1, data$totalCases)
summary(totalCases_ifelse)
```

###### **Using `pmax`**
```{r cache=TRUE}
# using pmax
totalCases_pmax <- pmax(data$totalCases, 1)
summary(totalCases_pmax)
```

###### **Using `subsetting`**
```{r cache=TRUE}
# subsetting
totalCases_subsetting <- data$totalCases
totalCases_subsetting[totalCases_subsetting < 1] <- 1
summary(totalCases_subsetting)
```

\

##### *Task 2*

###### Read the .sav file using foreign library which read.spss function

###### **For q01**
```{r cache=TRUE}

# read the .sav file using read_sav function from haven
saq_data <- read.spss("SAQ8.sav",to.data.frame=TRUE)


# for q1

q01 <- saq_data$q01

# computer mathematical operations
datalevels_q01 <- levels(q01)
freq_q01 <- as.numeric(table(q01))
percent_q01 <- as.numeric(round(prop.table(freq_q01) * 100, 1))
valid_percent_q01 <- as.numeric(round(prop.table(freq_q01) * 100, 1))
cum_percent <- cumsum(percent_q01)

# Create data frame
data <- data.frame(
  Levels = datalevels_q01,
  Freq = freq_q01,
  Percent = percent_q01,
  Val_Percent = valid_percent_q01,
  Cum_Percent = cum_percent
)

head(data)
```

```{r cache=TRUE}
# final version of calculated table for q01
data <- data %>% add_row(Levels = "Total", Freq = sum(data$Freq), 
                 Percent = sum(data$Percent), 
                 Val_Percent = sum(data$Val_Percent),
                 Cum_Percent = NULL)

# aethetics table using gt
data %>% gt(rowname_col = 'Levels') %>% 
  tab_header(title = md("Statistics makes me cry")) %>% 
  cols_label(Freq = "Frequency",
             Percent = "Percent",
             Val_Percent = "Valid Percent",
             Cum_Percent = "Cumulative Percent") %>% 
  sub_missing(missing_text = "")

```  

###### **For q03**

```{r cache=TRUE}

# extract q03

q03 <- saq_data$q03
datalevels_q03 <- levels(q03)
freq_q03 <- as.numeric(table(q03))
percent_q03 <- as.numeric(round(prop.table(freq_q03) * 100, 1))
valid_percent_q03 <- as.numeric(round(prop.table(freq_q03) * 100, 1))
cum_percent_q03 <- cumsum(percent_q03)

# convert the computed values into dataframe
data_q03 <- data.frame(
  Levels = datalevels_q03,
  Freq = freq_q03,
  Percent = percent_q03,
  Val_Percent = valid_percent_q03,
  Cum_Percent = cum_percent_q03
)

head(data_q03)
```

```{r cache=TRUE}

# add row for total
data_q03 <- data_q03 %>% add_row(Levels = "Total", 
                         Freq = sum(data_q03$Freq), 
                         Percent = sum(data_q03$Percent), 
                         Val_Percent = sum(data_q03$Val_Percent),
                         Cum_Percent = NULL)

# final version of calculated table
data_q03 %>% gt(rowname_col = 'Levels') %>% 
  tab_header(title = md("Statistic makes me cry")) %>% 
  cols_label(Freq = "Frequency",
             Percent = "Percent",
             Val_Percent = "Valid Percent",
             Cum_Percent = "Cumulative Percent") %>% 
  sub_missing(missing_text = "")

```

```{r cache=TRUE}

# extract q06
q06 <- saq_data$q06

datalevels_q06 <- levels(q06)
freq_q06 <- as.numeric(table(q06))
percent_q06 <- as.numeric(round(prop.table(freq_q06) * 100, 1))
valid_percent_q06 <- as.numeric(round(prop.table(freq_q06) * 100, 1))
cum_percent_q06 <- cumsum(percent_q06)

# convert into dataframe
data_q06 <- data.frame(
  Levels = datalevels_q06,
  Freq = freq_q06,
  Percent = percent_q06,
  Val_Percent = valid_percent_q06,
  Cum_Percent = cum_percent_q06
)
```

```{r cache=TRUE}

data_q06 <- data_q06 %>% add_row(Levels = "Total",
                         Freq = data_q06sum(data_q06$Freq), 
                         Percent = sum(data_q06$Percent), 
                         Val_Percent = sum(data_q06$Val_Percent),
                         Cum_Percent = NULL)

# final version of calculated table
data_q06 %>% gt(rowname_col = 'Levels') %>% 
  tab_header(title = md("I have little experience of computer")) %>% 
  cols_label(Freq = "Frequency",
             Percent = "Percent",
             Val_Percent = "Valid Percent",
             Cum_Percent = "Cumulative Percent") %>% 
  sub_missing(missing_text = "")
```

```{r cache=TRUE}
# for q08
q08 <- saq_data$q08

datalevels_q08 <- levels(q08)
freq_q08 <- as.numeric(table(q08))
percent_q08 <- as.numeric(round(prop.table(freq_q08) * 100, 2))
valid_percent_q08 <- as.numeric(round(prop.table(freq_q08) * 100, 2))
cum_percent_q08 <- cumsum(percent_q08)


data_q08 <- data.frame(
  Levels = datalevels_q08,
  Freq = freq_q08,
  Percent = round(valid_percent_q08,1),
  Val_Percent = round(valid_percent_q08,1),
  Cum_Percent = round(cum_percent_q08,1)
)

data_q08 <- data_q08 %>% add_row(Levels = "Total", Freq = sum(data_q06$Freq), 
                         Percent = sum(data_q08$Percent), 
                         Val_Percent = sum(data_q08$Val_Percent),
                         Cum_Percent = NULL)


# final version of calculated table
data_q08 %>% gt(rowname_col = 'Levels') %>% 
  tab_header(title = md("I have never been good at mathematics")) %>% 
  cols_label(Freq = "Frequency",
             Percent = "Percent",
             Val_Percent = "Valid Percent",
             Cum_Percent = "Cumulative Percent") %>% 
  sub_missing(missing_text = "")

```


#### **Task 2** Web Scraping
```{r cache=TRUE}
data_1 = 'https://data.covid19india.org/v4/min/timeseries.min.json'
data_2 = 'https://data.covid19india.org/v4/min/data.min.json'
covid_data_1 <- jsonlite::fromJSON(data_1)
covid_data_2 <- jsonlite::fromJSON(data_2)
```

```{r cache=TRUE}
covid_1_parsed <-
  covid_data_1 %>% enframe() %>% unnest_wider(value) %>% unnest_wider(dates) %>%
  pivot_longer(cols = !name,
               names_to = 'date',
               values_to = "value") %>% unnest_wider(value) %>%
  mutate(across(c(delta, delta7, total), ~ map(., ~ set_names(
    as_tibble(.x), paste0(cur_column(), "_", names(.))
  )))) %>%
  unnest_wider(c(delta, delta7, total))

covid_1_parsed[150:300, c("delta_confirmed","delta_recovered","delta_tested","delta7_confirmed","delta7_recovered","delta_tested")]
```

```{r cache=TRUE}
covid_2_parsed <- covid_data_2 %>% enframe() %>% unnest_wider(value) %>% 
  unnest_wider(c(delta,delta21_14, delta7, total),names_sep = "_") %>% select(-c(districts,meta))
covid_2_parsed[, c("delta_confirmed","delta_recovered","delta_tested","delta7_confirmed","delta7_recovered","delta_tested")]
```

```{r cache=TRUE}
merged_df <- merge(covid_1_parsed, covid_2_parsed, by = "name", all = FALSE)
head(merged_df[7250:8000,])

```

```{r cache=TRUE}

library(RSelenium)
library(rvest)
library(netstat)
rD <- rsDriver(browser="firefox",verbose = F, port = 14421L)
remDr <- rD[["client"]]
remDr$navigate("https://aqicn.org/forecast/kathmandu/")
aqi_html  <- read_html(remDr$getPageSource() %>% unlist())
aqi_html %>% html_element(".forecast-body-table") %>%  html_nodes("table") %>% html_table() -> forecast_table



aqi_table <- forecast_table %>% .[[1]]


aqi_table <- aqi_table %>% select(-c('X10','X11','X20','X21','X30','X31','X40','X41','X50','X51','X60','X61'))

aqi_table <- aqi_table %>% filter(X1 != 'UVI')
aqi_table <- aqi_table %>% filter(X1 != 'humidity')
aqi_table <- aqi_table %>% mutate(X1 = replace(X1, 9, "humidity"))
aqi_table <- aqi_table %>% mutate(X1 = replace(X1, 1, "Index"))
aqi_table <- aqi_table %>% filter(X1 != '')

headers <- aqi_table[1,]

colnames(aqi_table) <- headers

aqi_table <- aqi_table[-1,]

aqi_table <- aqi_table %>% column_to_rownames(var = 'Index')

library(stringr)

aqi_table[2,] <- floor(as.integer(str_extract(as.character(aqi_table[2,]), "\\d+")) / 1000)
aqi_table[3,] <- floor(as.integer(str_extract(as.character(aqi_table[3,]), "\\d+")) / 100)

lengths <- as.numeric(nchar(aqi_table[4,]))
aqi_table[4,] <- ifelse(lengths == 2, substr(aqi_table[4,], 1, 1), ifelse(lengths %in% 3:4, substr(aqi_table[4,], 1, 2), ""))

aqi_table
```

```{r cache=TRUE}
files <- list.files(pattern = "pdf$")
files
```




#### **Task 3**

```{r cache=TRUE}
files <- list.files(pattern = "pdf$")
files
```


```{r cache=TRUE}
# load the pdf files into list
pdf_files <- lapply(files, pdf_text)
```



```{r cache=TRUE}

# create a corpus from vector source i.e from list pdf_files
corpus <- Corpus(VectorSource(unlist(pdf_files)))

# make a duplicate of the loaded corpus for future use
corpus_copy <- corpus

```


#### Preprocessing Corpus 

```{r cache=TRUE, warning=FALSE}

# convert the all texts in lower
corpus <- tm_map(corpus, tolower)

# remove punctuations
corpus <- tm_map(corpus, removePunctuation)

# remove numbers
corpus <- tm_map(corpus, removeNumbers)

my_stopwords <- c("can","may","used")
# remove stopwords from the corpus
corpus <- tm_map(corpus, removeWords, my_stopwords)

# stem the corpus
corpus <- tm_map(corpus, stemDocument)

# since values and value are same so replaced values and value
remove <- function(x) gsub("values","value",x)
corpus <-  tm_map(corpus, remove)

head(corpus)
```


#### Term Document Matrix 
```{r cache=TRUE}
# create Term Document Matrix with word lenght 1 or many
tdm <- TermDocumentMatrix(corpus, control = list((wordLenghts=c(1,Inf))))
head(tdm)
```


#### Best way to create TDM with less code
```{r cache=TRUE, warning=FALSE}
remove <- function(x) gsub("values","value",x)
corpus_copy <-  tm_map(corpus_copy, remove)
my_tdm <- TermDocumentMatrix(
  corpus_copy,
  control =
    list(
      removePunctuation = TRUE,
      stopwords = TRUE,
      tolower = TRUE,
      stemming = FALSE,
      removeNumbers = TRUE,
      bounds = list(global = c(3, Inf)),
      wordLenghts = c(1,Inf),
      removeWords = (c("can","may","used")))
)
```


#### Frequency Terms 
```{r cache=TRUE}
# finding frequency of words which is at least present 10 times
low_frequent_terms <- findFreqTerms(my_tdm, lowfreq = 10)
head(low_frequent_terms)

# finding frequency of words which is at max present 10 times
high_frequent_terms <- findFreqTerms(my_tdm, highfreq = 10)
head(high_frequent_terms)
```


#### Word Association
```{r cache=TRUE}
findAssocs(my_tdm, "mining", 0.3)
findAssocs(my_tdm, "learning", 0.35)
findAssocs(my_tdm, "classification", 0.4)
```


#### Top words in TDM
```{r cache=TRUE}
# top 10 words and their respective counts 
df <-
    my_tdm %>%
    as.matrix() %>%
    rowSums() %>%
    sort(decreasing = TRUE) %>%
    head(10) %>%
    enframe(name = "word", value = "counts")

head(df)
```


#### Bar Grahph for Top 10 word counts
```{r cache=TRUE}
# top 10 words and counts using bargraph
bargraph <- ggplot(df, aes(word, counts)) +
  geom_bar(stat = "identity", fill = "#E69F00") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Top 10 words by counts.") +
  geom_text(aes(label = counts), vjust = -0.5)
bargraph
```

```{r cache=TRUE}
mat <- as.matrix(my_tdm)
freq <- mat %>% rowSums() %>% sort(decreasing = T)

# plot word cloud
wordcloud(
  words = names(freq),
  freq = freq,
  min.freq = 300,
  max.words = 500,
  random.order = FALSE,
  colors = brewer.pal(8, "Dark2"),
  random.color = TRUE,
  rot.per = 0.35,
  use.r.layout = FALSE
)
```

#### Word Correlation
```{r cache=TRUE}
# correlation between top 600 frequent terms
top_600_frequent_tems <- findFreqTerms(my_tdm, lowfreq = 650)
plot(my_tdm, terms = top_600_frequent_tems, corThreshold = 0.2, weighting = T)

```

